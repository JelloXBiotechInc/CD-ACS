{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14820e7e-e3be-4f9f-9719-e5e193a9ef29",
   "metadata": {},
   "source": [
    "# CDACS Model Experiment - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35752097-ba0f-4fb3-8542-65760cfc523e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5b0f3-05fe-4469-8c61-9cc4ecc14aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de20cc-612b-49f0-a1fa-b07c665b67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736ae25-edb8-4e84-b7c9-87738b2532fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.model_utils import *\n",
    "from module.metrics import *\n",
    "from module.dataset_utils import BasicDatasetProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5fe57-a56b-4d8a-9d90-1f3b6c73cf2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79790235-0370-48f2-b1c0-b614c0d037fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets.camelyon16\n",
    "\n",
    "dataset_wrappers_he = BasicDatasetProcess.get_dataset_wrapper_from_dataset('camelyon16', 'HE_CR')\n",
    "\n",
    "dataset_wrappers_he"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb93f5-9eb0-4969-9b7f-3445ba624101",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b992e231-077b-47dc-aed8-291243f4ba95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define batch preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5555f-3fc5-4419-b9f1-243bb66f88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training proportion for seperate train data into trainging|validation\n",
    "train_proportion = 0.7\n",
    "\n",
    "# patch size for WSI before input into model\n",
    "patch_size = 1000\n",
    "\n",
    "# input size for model\n",
    "# e.g. patch_size=1000, input_size = 1024\n",
    "# A WSI for 10000x10000 pixels size will patched into 100 patched with 1000x1000 size\n",
    "# multiple patches for 1000x1000 size will be resized into 1024x1024\n",
    "input_size = 1024\n",
    "# number of patches for random patch during training\n",
    "num_patches = 100\n",
    "\n",
    "\n",
    "# if WSI too big for training, try turning off\n",
    "cache = False\n",
    "prefetch = False\n",
    "\n",
    "\n",
    "# number of patches input into model at the same time\n",
    "BATCH_SIZE = 10\n",
    "# buffer size for shuffling\n",
    "BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006d9a8-0fc3-4286-b942-31ac7bb6c44e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preprocessing dataset using Color Deconvolution(CD) algorithm and Adaptive Color Segmentation(ACS) a.k.a. Color Region(CR) algorithm in the batch-processed manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349b732-f8e1-44b2-a730-669b760924ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "he_train_pre, he_val_pre = dataset_wrappers_he['train'].random_split(train_proportion)\n",
    "he_train = he_train_pre.process().cd_normalize()\n",
    "he_val = he_val_pre.process().cd_normalize()\n",
    "\n",
    "train_dataset = he_train\n",
    "val_dataset = he_val\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e481aef-daa8-49d1-8ec6-17538e7b550a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Prepare training dataset with randomly patching subimages for preventing artifacts in Image Segmentation training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c5382-e102-4bcd-9331-3aae26954497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "train_images = train_dataset \\\n",
    "    .unpack_datapoint() \\\n",
    "    .assert_callback(lambda ds: ds.cache() if cache else ds) \\\n",
    "    .random_patches(num_patches, patch_size) \\\n",
    "    .resize_image(input_size)\n",
    "print(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4125b-ee8c-49ba-ab21-b708660f8ce8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Prepare validation dataset with ordered patching subimages for complete validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbcf31-ca6a-446e-a3b3-d2889764dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_dataset)\n",
    "val_images = val_dataset \\\n",
    "    .unpack_datapoint() \\\n",
    "    .extract_large_patches(patch_size) \\\n",
    "    .resize_image(input_size) \\\n",
    "    .assert_callback(lambda ds: ds.cache() if cache else ds)\n",
    "print(val_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7da57-9ca0-445a-a439-c5ec3bc5af8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a547ae-3901-465f-80db-b0d0424fb7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = train_images.dataset_size\n",
    "VALIDATION_LENGTH = val_images.dataset_size\n",
    "\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26704e8-8e3e-4abc-8697-9dab6e1f3fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visually verify prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acdffe-2ab6-4352-9d39-8e8aadbc6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in he_val.unpack_datapoint().processed_dataset.shuffle(10).take(10):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "    if Dataset.get_ratio(sample_mask) >= 1e-2:\n",
    "        display([sample_image, sample_mask])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53650d89-8d54-4540-a949-cb0c3d7dcade",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data augmentations using tf built-in functions to further preventing artifacts from image-segmentation-training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7cec9-e7ea-43f6-bece-26befade6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(input_image, input_mask):\n",
    "    # flipping random horizontal or vertical\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_up_down(input_image)\n",
    "        input_mask = tf.image.flip_up_down(input_mask)\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5c3bc-ead0-4316-998a-db01defd1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images)\n",
    "train_batches = (\n",
    "    train_images\n",
    "    .processed_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(augment)\n",
    ")\n",
    "if prefetch:\n",
    "    train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "print(train_batches)\n",
    "\n",
    "print(val_images)\n",
    "val_batches = (\n",
    "    val_images\n",
    "    .processed_dataset\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "if prefetch:\n",
    "    val_batches = val_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "print(val_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d608e77-d6c4-4d84-9de3-b8874778cdbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup UNet + MobileNetV2 hybrid model for 1024 input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131369f-253a-459d-94e9-0cdb444c64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# optimizer = Adam(1e-4)\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf240d-86a9-47e3-881d-1aaec063efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = MobileNetV2_1024_Model(\n",
    "    output_channels=2,\n",
    "    input_channels=1,\n",
    "    input_size=input_size,\n",
    ")\n",
    "model = model_obj.model\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\n",
    "                  'accuracy',\n",
    "                  jacard_coef,\n",
    "                  dice_coef,\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64aae47-e1fa-4b31-9520-60a670823dfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Resume previous training (optional)\n",
    "- Set cell below to `code` type\n",
    "- Uncomment `initial_epoch` parameter in model.fit to resume actual epoch count"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12aaf321-7256-4aaa-b881-d12ff52261c8",
   "metadata": {},
   "source": [
    "weight_path = '/path/to/your/weight.hdf5'\n",
    "\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\n",
    "    weight_path,\n",
    "    custom_objects={\n",
    "        'jacard_coef': jacard_coef,\n",
    "        'dice_coef': dice_coef,\n",
    "    }\n",
    ")\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\n",
    "                  'accuracy',\n",
    "                  jacard_coef,\n",
    "                  dice_coef,\n",
    "              ])\n",
    "model_obj.model = model\n",
    "initial_epoch = model.optimizer.iterations.numpy() // STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2c9e6-73d2-438b-9c43-f4a90d4c15a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Define serious of callback functions that will be used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe23c05-7308-47c1-a9bf-7239fb6fd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_prediction(image, mask, patch_size=patch_size):\n",
    "    pred_mask = model_obj.easy_predict_single(image, patch_size=patch_size)\n",
    "    return image, mask, pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76195716-c92c-4789-b9b4-983d17273e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_image, n_mask, pred_mask = large_prediction(sample_image, sample_mask)\n",
    "display([sample_image, sample_mask, pred_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6a7d4-bf2b-4f57-b8ef-93812d5fa250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        img, mask, pred_mask = large_prediction(sample_image, sample_mask)\n",
    "        fig = display([img, mask, pred_mask], show=False)\n",
    "        fig.savefig(os.path.join(output_folder, f'pred_sample_image_epoch_{epoch:04d}.png'))\n",
    "        \n",
    "        plt.show()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac55e6c-b5bc-4822-83ba-ccda058dcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'checkpoints'\n",
    "logdir        = \"logs/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "best_checkpoint_filepath = output_folder+\"/model_epoch_{epoch:04d}_val_dict_{val_dice_coef:.5f}.hdf5\"\n",
    "model_best_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_dice_coef',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "checkpoint_filepath = output_folder+\"/model_epoch_{epoch:04d}.hdf5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose=0,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq='epoch',\n",
    "    period=10,\n",
    ")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4065a-6108-49c5-be6e-b9bfed39491f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa2287-f55d-46a7-87d0-512cc7a1d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "VALIDATION_STEPS = VALIDATION_LENGTH//BATCH_SIZE\n",
    "\n",
    "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_data=val_batches,\n",
    "                          # initial_epoch=initial_epoch,\n",
    "                          callbacks=[\n",
    "                              DisplayCallback(),\n",
    "                              tensorboard_callback,\n",
    "                              model_best_checkpoint_callback,\n",
    "                              model_checkpoint_callback,\n",
    "                          ]\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
